"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –º–µ–∂–¥—É models.py –∏ –ë–î.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python check_model_duplicates.py
"""

import sys
from pathlib import Path
from typing import Dict, List, Set
import re

# –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
PROJECT_ROOT = Path(r"C:\ProjectF\field-service")
MODELS_FILE = PROJECT_ROOT / "field_service" / "db" / "models.py"
ALEMBIC_VERSIONS = PROJECT_ROOT / "alembic" / "versions"

# –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∞—è —Å—Ö–µ–º–∞ –∏–∑ ALL_BD.md
CANONICAL_SCHEMA = {
    "orders": {
        "fields": {
            "id", "city_id", "district_id", "street_id", "house", "apartment",
            "address_comment", "client_name", "client_phone", "status",
            "preferred_master_id", "assigned_master_id", "created_by_staff_id",
            "created_at", "updated_at", "version", "company_payment",
            "guarantee_source_order_id", "order_type", "category", "description",
            "late_visit", "dist_escalated_logist_at", "dist_escalated_admin_at",
            "lat", "lon", "timeslot_start_utc", "timeslot_end_utc", "total_sum",
            "cancel_reason", "no_district", "type", "geocode_provider",
            "geocode_confidence", "escalation_logist_notified_at",
            "escalation_admin_notified_at"
        },
        "fks": {
            "city_id": "cities.id",
            "district_id": "districts.id",
            "street_id": "streets.id",
            "preferred_master_id": "masters.id",
            "assigned_master_id": "masters.id",
            "created_by_staff_id": "staff_users.id",
            "guarantee_source_order_id": "orders.id"
        }
    },
    "commissions": {
        "fields": {
            "id", "order_id", "master_id", "amount", "percent", "status",
            "deadline_at", "paid_at", "blocked_applied", "blocked_at",
            "payment_reference", "created_at", "updated_at", "rate",
            "paid_reported_at", "paid_approved_at", "paid_amount", "is_paid",
            "has_checks", "pay_to_snapshot"
        },
        "fks": {
            "order_id": "orders.id",
            "master_id": "masters.id"
        },
        "constraints": {
            "unique": ["order_id"]
        }
    },
    "offers": {
        "fields": {
            "id", "order_id", "master_id", "round_number", "state", "sent_at",
            "responded_at", "expires_at", "created_at"
        },
        "fks": {
            "order_id": "orders.id",
            "master_id": "masters.id"
        }
    },
    "staff_access_codes": {
        "fields": {
            "id", "code", "role", "city_id", "created_by_staff_id",
            "used_by_staff_id", "expires_at", "used_at", "created_at",
            "comment", "revoked_at"
        },
        "fks": {
            "city_id": "cities.id",
            "created_by_staff_id": "staff_users.id",
            "used_by_staff_id": "staff_users.id"
        }
    }
}


def extract_model_fields(model_text: str) -> Set[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–∞ –ø–æ–ª–µ–π –∏–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."""
    fields = set()
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è Mapped –ø–æ–ª–µ–π
    pattern = r'(\w+):\s*Mapped\[.*?\]\s*=\s*mapped_column'
    for match in re.finditer(pattern, model_text):
        fields.add(match.group(1))
    return fields


def check_model_consistency(model_name: str, model_text: str) -> List[str]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–π —Å—Ö–µ–º–µ."""
    issues = []
    
    if model_name not in CANONICAL_SCHEMA:
        return issues
    
    canonical = CANONICAL_SCHEMA[model_name]
    actual_fields = extract_model_fields(model_text)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–ª–µ–π
    missing_fields = canonical["fields"] - actual_fields
    if missing_fields:
        issues.append(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è: {', '.join(sorted(missing_fields))}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏—à–Ω–∏—Ö –ø–æ–ª–µ–π (–º–æ–≥—É—Ç –±—ã—Ç—å –∞–ª–∏–∞—Å–∞–º–∏ - —ç—Ç–æ OK)
    extra_fields = actual_fields - canonical["fields"]
    if extra_fields:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è—é—Ç—Å—è –ª–∏ –æ–Ω–∏ synonym
        non_synonym_extras = []
        for field in extra_fields:
            if not re.search(rf'{field}\s*=\s*synonym\(', model_text):
                non_synonym_extras.append(field)
        
        if non_synonym_extras:
            issues.append(f"  ‚ö†Ô∏è  –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (–Ω–µ –∞–ª–∏–∞—Å—ã): {', '.join(sorted(non_synonym_extras))}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ FK
    if "fks" in canonical:
        for fk_field, fk_target in canonical["fks"].items():
            # –ò—â–µ–º ForeignKey –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –ø–æ–ª—è
            fk_pattern = rf'{fk_field}:\s*Mapped.*?ForeignKey\(["\']({fk_target})["\']'
            if not re.search(fk_pattern, model_text):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ—Ç –±—ã—Ç—å FK –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –±–µ–∑ —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ Integer)
                int_pattern = rf'{fk_field}:\s*Mapped.*?mapped_column\(\s*(?:Integer|BigInteger)'
                if re.search(int_pattern, model_text):
                    issues.append(f"  ‚ùå –ü–æ–ª–µ {fk_field} –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å FK –Ω–∞ {fk_target}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ unique constraints
    if "constraints" in canonical and "unique" in canonical["constraints"]:
        for unique_field in canonical["constraints"]["unique"]:
            if not re.search(rf'{unique_field}.*unique\s*=\s*True', model_text):
                issues.append(f"  ‚ö†Ô∏è  –ü–æ–ª–µ {unique_field} –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å unique")
    
    return issues


def find_migration_duplicates() -> Dict[str, List[str]]:
    """–ò—â–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ç–∞–±–ª–∏—Ü –≤ –º–∏–≥—Ä–∞—Ü–∏—è—Ö."""
    duplicates = {}
    
    if not ALEMBIC_VERSIONS.exists():
        return duplicates
    
    table_creates = {
        "orders": [],
        "commissions": [],
        "offers": [],
        "staff_access_codes": []
    }
    
    for migration_file in ALEMBIC_VERSIONS.glob("*.py"):
        content = migration_file.read_text(encoding="utf-8")
        
        # –ò—â–µ–º op.create_table –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü—ã
        for table_name in table_creates.keys():
            pattern = rf'op\.create_table\(\s*["\']({table_name})["\']'
            if re.search(pattern, content):
                table_creates[table_name].append(migration_file.name)
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–∞–±–ª–∏—Ü—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å–æ–∑–¥–∞–Ω–∏—è–º–∏
    for table_name, files in table_creates.items():
        if len(files) > 1:
            duplicates[table_name] = files
    
    return duplicates


def main():
    print("=" * 80)
    print("–ü–†–û–í–ï–†–ö–ê –ú–û–î–ï–õ–ï–ô –ò –î–£–ë–õ–ò–ö–ê–¢–û–í")
    print("=" * 80)
    print()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ models.py
    if not MODELS_FILE.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {MODELS_FILE}")
        sys.exit(1)
    
    print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞: {MODELS_FILE}")
    print()
    
    # –ß–∏—Ç–∞–µ–º models.py
    models_content = MODELS_FILE.read_text(encoding="utf-8")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
    total_issues = 0
    for model_name in CANONICAL_SCHEMA.keys():
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞
        pattern = rf'class {model_name}\(Base\):.*?(?=\nclass\s|\n\n# =====|\Z)'
        match = re.search(pattern, models_content, re.DOTALL)
        
        if not match:
            print(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ models.py")
            print()
            continue
        
        model_text = match.group(0)
        issues = check_model_consistency(model_name, model_text)
        
        if issues:
            print(f"üìã –ú–æ–¥–µ–ª—å: {model_name}")
            for issue in issues:
                print(issue)
            print()
            total_issues += len(issues)
        else:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ö–µ–º–µ")
            print()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –º–∏–≥—Ä–∞—Ü–∏—è—Ö
    print("=" * 80)
    print("–ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í –í –ú–ò–ì–†–ê–¶–ò–Ø–•")
    print("=" * 80)
    print()
    
    duplicates = find_migration_duplicates()
    
    if duplicates:
        print("‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü:")
        print()
        for table_name, files in duplicates.items():
            print(f"  –¢–∞–±–ª–∏—Ü–∞ '{table_name}' —Å–æ–∑–¥–∞–µ—Ç—Å—è –≤ {len(files)} –º–∏–≥—Ä–∞—Ü–∏—è—Ö:")
            for file in files:
                print(f"    - {file}")
            print()
    else:
        print("‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –º–∏–≥—Ä–∞—Ü–∏—è—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        print()
    
    # –ò—Ç–æ–≥–∏
    print("=" * 80)
    print("–ò–¢–û–ì–ò")
    print("=" * 80)
    print()
    print(f"–í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º –≤ models.py: {total_issues}")
    print(f"–¢–∞–±–ª–∏—Ü —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏ –≤ –º–∏–≥—Ä–∞—Ü–∏—è—Ö: {len(duplicates)}")
    print()
    
    if total_issues > 0 or duplicates:
        print("‚ö†Ô∏è  –¢—Ä–µ–±—É–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
        print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç 'models_patch' –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π")
        sys.exit(1)
    else:
        print("‚úÖ –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        sys.exit(0)


if __name__ == "__main__":
    main()
